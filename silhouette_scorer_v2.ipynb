{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/cpenelto/miniconda3/envs/candice-base/lib/python3.9/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n",
      "2022-02-21 17:20:55.056797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-21 17:20:55.056850: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join,isfile\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr,spearmanr,percentileofscore\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 12\n",
    "min_samples = 10\n",
    "embedding_dir = join('data','embeddings')\n",
    "filter_labels = {'Evans Early American Imprints','HeinOnline','National Archives Founders Online'}\n",
    "#filter_labels = {'Evans Early American Imprints':0,'HeinOnline':1,'National Archives Founders Online':2}\n",
    "file_header  = \"cofea_sampled_vectors_\"\n",
    "embedding_files = [f for f in listdir(embedding_dir) if isfile(join(embedding_dir, f))\n",
    "                  and '.dict' in f]\n",
    "indir = join('data','preprocessed')\n",
    "outdir = join('data','preprocessed')\n",
    "special_terms_file = join('data','interest_terms.txt')\n",
    "target_index_file = join(indir,'sample_target_index.dict')\n",
    "data_file = join(indir, 'cofea.jsonlist')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file info for cofea\n",
    "with open(data_file) as f:\n",
    "     cofea_data = f.readlines()\n",
    "# get the document index of the embeddings\n",
    "with open(target_index_file,'rb') as f:\n",
    "    target_index = pickle.load(f)\n",
    "    \n",
    "# list of terms of interest\n",
    "with open(special_terms_file, 'r',encoding = 'utf-8') as f:\n",
    "    special_terms = f.read().splitlines()\n",
    "    \n",
    "# they were indexed and saved in their tokenized form \n",
    "special_terms = [tokenizer.tokenize(x) for x in special_terms]\n",
    "\n",
    "special_terms_cleaned = []\n",
    "for x in special_terms:\n",
    "    # rejoin into concatenated words\n",
    "    rejoined_pieces = []\n",
    "    for p_i, piece in enumerate(x):\n",
    "        if p_i == 0:\n",
    "            rejoined_pieces.append(piece)\n",
    "        elif piece.startswith('##'):\n",
    "            rejoined_pieces[-1] += piece\n",
    "        else:\n",
    "            rejoined_pieces.append(piece)\n",
    "    special_terms_cleaned.append(' '.join(rejoined_pieces))\n",
    "special_terms_cleaned = set(special_terms_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1088/1088 [1:23:13<00:00,  4.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Variation Across Subsets\n",
    "# measure silhouette scores using the subset (Founders, Evans, or Hein) as labels\n",
    "# those that have high silhouette scores have very different contexts in the various subsets\n",
    "# again, look at those terms that vary the most, and terms of interest that \n",
    "# have high silhouette scores (in terms of quantiles), vs those that are low\n",
    "\n",
    "# provide labels to embeddings based on source\n",
    "subset_eval_scores = {}\n",
    "subset_target_labels = {}\n",
    "\n",
    "for target_file in tqdm(embedding_files):\n",
    "    target_word = target_file.replace(file_header,'')\n",
    "    target_word = target_word.replace('.dict','')\n",
    "    # get embeddings\n",
    "    with open(join(embedding_dir,target_file),'rb') as f:\n",
    "        target_embeddings = pickle.load(f)\n",
    "    embeddings = target_embeddings[layer]\n",
    "    # get labels and only save embeddings that are specific sources\n",
    "    labels = []\n",
    "    filter_embeddings = []\n",
    "    for x,index in enumerate(target_index[target_word]):\n",
    "        _,doc_index,_ = index\n",
    "        doc = cofea_data[doc_index]\n",
    "        doc = json.loads(doc)\n",
    "        source = doc['source']\n",
    "        if source in filter_labels:\n",
    "            labels.append(source)\n",
    "            filter_embeddings.append(embeddings[x])\n",
    "    # get the silhouette score\n",
    "    if len(filter_embeddings ) > min_samples and len(set(labels))>1:\n",
    "        X = np.array(filter_embeddings)\n",
    "        # save the clustering labels and scores\n",
    "        subset_eval_scores[target_word] = silhouette_score(X, labels)\n",
    "        subset_target_labels[target_word] = labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(outdir,'sillhouette_scores_subset_variation.dict'),'wb') as f:\n",
    "    pickle.dump(subset_eval_scores,file=f)\n",
    "with open(join(outdir,'subset_labels.dict'),'wb') as f:\n",
    "    pickle.dump(subset_target_labels,file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1088/1088 [43:36<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Variation over all\n",
    "# use the same contextual embeddings and pool Evans Hein and Founders together\n",
    "# use k-means clustering with k=2, then compute silhouette scores on those clusters\n",
    "# this will help to pick out those that have at least two distinct meanings\n",
    "# may want to further split those about some silhouette threshold and check the silhouette scores of the resulting clusters\n",
    "# again can identify terms that seem to show a lot vs a little variation in usage / contexts (again with a focus on terms of interest that might require more investigation)\n",
    "\n",
    "\n",
    "overall_eval_scores = {}\n",
    "overall_target_labels = {}\n",
    "for target_file in tqdm(embedding_files):\n",
    "    target_word = target_file.replace(file_header,'')\n",
    "    target_word = target_word.replace('.dict','')\n",
    "\n",
    "    with open(join(embedding_dir,target_file),'rb') as f:\n",
    "        target_embeddings = pickle.load(f)\n",
    "        \n",
    "    target_embeddings = target_embeddings[layer]\n",
    "    if len(target_embeddings ) > min_samples:\n",
    "        X = np.array(target_embeddings)\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "        # check if there are at least 4 items in each cluster\n",
    "        c1 = 0\n",
    "        for x in kmeans.labels_:\n",
    "            if x == 0:\n",
    "                c1 += 1\n",
    "                \n",
    "        if c1 >= 4 :\n",
    "            # sihouette score\n",
    "            overall_eval_scores[target_word] = silhouette_score(X, kmeans.labels_)\n",
    "        \n",
    "        \n",
    "        # save the clustering labels\n",
    "        overall_target_labels[target_word] = kmeans.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(outdir,'sillhouette_scores_over_all_variation.dict'),'wb') as f:\n",
    "    pickle.dump(overall_eval_scores,file=f)\n",
    "with open(join(outdir,'kmeans_labels.dict'),'wb') as f:\n",
    "    pickle.dump(overall_target_labels,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of scores\n",
    "eval_x = [eval_scores[x] for x in eval_scores]\n",
    "num_bins  = 15\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.set_xlabel('silhouette score')\n",
    "n, bins, patches = ax.hist(eval_x, num_bins, facecolor='blue', alpha=0.5)\n",
    "# plt.title(r' Silhouette Score Histogram for evans')\n",
    "#plt.savefig('eval_silhouette_hist')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Evans Early American Imprints' in filter_labels:\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
