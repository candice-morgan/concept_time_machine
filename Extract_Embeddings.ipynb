{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b63362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 23:56:00.353127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-04 23:56:00.353176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf960ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = os.path.join('data','preprocessed')\n",
    "sample_file = os.path.join('data','preprocessed','sample_target_index.dict')\n",
    "outdir = os.path.join('data','preprocessed')\n",
    "batch_size = 200\n",
    "layers = '10,11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ab3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect index of tokens in the documents\n",
    "files = sorted(glob(os.path.join(in_dir, '*_tokenized.jsonlist')))\n",
    "docs = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        docs.append(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc56b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample index\n",
    "with open(sample_file,'rb') as f:\n",
    "    sample_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217c003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#layers\n",
    "layers = [int(layer) for layer in layers.split(',')]\n",
    "\n",
    "# load the model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# move the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc00f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(token_ids, target_position, sequence_length=128):\n",
    "    \"\"\"\n",
    "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
    "    (and the target word's position in the snippet).\n",
    "    :param token_ids: list of token ids (for an entire line of text)\n",
    "    :param target_position: index of the target word's position in `tokens`\n",
    "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
    "    :return: (context_ids, new_target_position)\n",
    "                context_ids: list of token ids for the output sequence\n",
    "                new_target_position: index of the target word's position in `context_ids`\n",
    "    \"\"\"\n",
    "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
    "    window_size = int((sequence_length - 2) / 2)\n",
    "    context_start = max([0, target_position - window_size])\n",
    "    padding_offset = max([0, window_size - target_position])\n",
    "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
    "\n",
    "    context_ids = token_ids[context_start:target_position + window_size]\n",
    "    context_ids += padding_offset * [0]\n",
    "\n",
    "    new_target_position = target_position - context_start\n",
    "\n",
    "    return context_ids, new_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3b29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just put the output into some lists for now\n",
    "segment_id_list = []\n",
    "token_list = []\n",
    "piece_index_list = []\n",
    "token_index_list = []\n",
    "vectors_by_layer = defaultdict(list)\n",
    "to_encode = []\n",
    "encoded = []\n",
    "targets = []\n",
    "token_indices = []\n",
    "lengths = []\n",
    "line_ids = []\n",
    "\n",
    "for target_word in sample_index:\n",
    "    for file_id,doc_id,index in sample_index[target_word]:\n",
    "        doc = json.loads(docs[file_id][doc_id])\n",
    "        tokens = doc['tokens']\n",
    "        # now we get the context\n",
    "        context_ids, pos_in_context = get_context(tokens, index)\n",
    "        input_ids = ['[CLS]']+context_ids+['[SEP]']\n",
    "        encoded.append(input_ids)\n",
    "        break\n",
    "    break\n",
    "        #if len(to_encode) == batch_size or (line_index == (n_lines-1) and len(to_encode) > 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0b7d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  ',',\n",
       "  'sixteen',\n",
       "  'others',\n",
       "  ',',\n",
       "  'spread',\n",
       "  'over',\n",
       "  'a',\n",
       "  'country',\n",
       "  'of',\n",
       "  'two',\n",
       "  'thousand',\n",
       "  'miles',\n",
       "  'diameter',\n",
       "  ',',\n",
       "  'rise',\n",
       "  'up',\n",
       "  'on',\n",
       "  'every',\n",
       "  'side',\n",
       "  ',',\n",
       "  'ready',\n",
       "  'organised',\n",
       "  ',',\n",
       "  'for',\n",
       "  'del',\n",
       "  '##ibe',\n",
       "  '##ration',\n",
       "  'by',\n",
       "  'a',\n",
       "  'constitutional',\n",
       "  'legislature',\n",
       "  ',',\n",
       "  '&',\n",
       "  'for',\n",
       "  'action',\n",
       "  'by',\n",
       "  'their',\n",
       "  'governor',\n",
       "  ',',\n",
       "  'constitutional',\n",
       "  '##ly',\n",
       "  'the',\n",
       "  'commander',\n",
       "  'of',\n",
       "  'the',\n",
       "  'militia',\n",
       "  'of',\n",
       "  'the',\n",
       "  'state',\n",
       "  ',',\n",
       "  'that',\n",
       "  'is',\n",
       "  'to',\n",
       "  'say',\n",
       "  ',',\n",
       "  'of',\n",
       "  'every',\n",
       "  'man',\n",
       "  'in',\n",
       "  'it',\n",
       "  ',',\n",
       "  'able',\n",
       "  'to',\n",
       "  'bear',\n",
       "  'arms',\n",
       "  ';',\n",
       "  'and',\n",
       "  'that',\n",
       "  'militia',\n",
       "  'too',\n",
       "  'regularly',\n",
       "  'formed',\n",
       "  'into',\n",
       "  'regiments',\n",
       "  '&',\n",
       "  'battalions',\n",
       "  ',',\n",
       "  'into',\n",
       "  'infantry',\n",
       "  ',',\n",
       "  'cavalry',\n",
       "  '&',\n",
       "  'artillery',\n",
       "  ',',\n",
       "  'trained',\n",
       "  'under',\n",
       "  'officers',\n",
       "  'general',\n",
       "  '&',\n",
       "  'subordinate',\n",
       "  ',',\n",
       "  'legally',\n",
       "  'appointed',\n",
       "  ',',\n",
       "  'always',\n",
       "  'in',\n",
       "  'readiness',\n",
       "  ',',\n",
       "  'and',\n",
       "  'to',\n",
       "  'whom',\n",
       "  'they',\n",
       "  'are',\n",
       "  'already',\n",
       "  'in',\n",
       "  'habits',\n",
       "  'of',\n",
       "  'obedience',\n",
       "  '.',\n",
       "  'the',\n",
       "  'republican',\n",
       "  'government',\n",
       "  'of',\n",
       "  'france',\n",
       "  'was',\n",
       "  'lost',\n",
       "  'without',\n",
       "  'a',\n",
       "  'struggle',\n",
       "  ',',\n",
       "  'because',\n",
       "  'the',\n",
       "  'party',\n",
       "  'of',\n",
       "  '‘',\n",
       "  '’',\n",
       "  '[SEP]']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e7be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
