{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim,logging\n",
    "import json\n",
    "import os\n",
    "from os import listdir,makedirs\n",
    "from os.path import isfile, join,exists\n",
    "import json\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cofea dataset\n",
    "cofea_dir = \"cofea_full\"\n",
    "cofea_files = [f for f in listdir(cofea_dir) if isfile(join(cofea_dir, f)) and \".json\" in f]\n",
    "cofea_full = []\n",
    "for file in cofea_files:\n",
    "    with open(cofea_dir+\"/\"+file, encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        cofea_full.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break int sencences and tokenize\n",
    "sentences = []\n",
    "# relying on dict implementation for speed\n",
    "for file in cofea_full:\n",
    "    for doc in file:\n",
    "        doc_sp = nlp(doc['body'].strip())\n",
    "        tokenized_sents = []\n",
    "        for sent in doc_sp.sents:\n",
    "            tokenized_sents.append([token.text for token in sent])\n",
    "        sentences.extend(tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in cofea_full:\n",
    "    for doc in file:\n",
    "        doc = doc['body'].strip()\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may use l\n",
    "nlp.max_length = 6000000\n",
    "#enabled = nlp.select_pipes(disable=['ner'])\n",
    "sentences = []\n",
    "for doc in nlp.pipe(docs[:3],batch_size=100):\n",
    "    #sentences.append([token.text for token in doc])\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text for token in sent ]#if not bool(re.match('^(\\\\n)+$',token.text))]\n",
    "        if len(tokens) != 0:\n",
    "            sentences.append(tokens)\n",
    "\n",
    "\n",
    "\n",
    "#nlp enable to only use tokenizer and sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Page', '\\n\\n', 'THE', '\\n', 'DEBATES', '\\n']\n",
      "['IN', 'THE', 'SEVERAL', '\\n', 'STATE', 'CONVENTIONS', ',']\n",
      "['\\n', 'ON', 'THE', 'ADOPTION', 'OF', 'THE', '\\n', 'FEDERAL', 'CONSTITUTION', ',', '\\n', 'AS', 'RECOMMENDED', 'BY', 'THE', '\\n', 'GENERAL']\n",
      "['CONVENTION']\n",
      "['AT', 'PHILADELPHIA', ',', '\\n', 'IN', '\\n', '1787', '.']\n",
      "['\\n']\n",
      "['TOGETHER', 'WITH', 'THE', '\\n', 'JOURNAL', 'OF', 'THE', 'FEDERAL', 'CONVENTION', ',', '\\n', 'LUTHER', 'MARTIN', \"'S\", 'LETTER', ',', '\\n', 'YATES', \"'S\", 'MINUTES', ',', '\\n', 'CONGRESSIONAL', 'OPINIONS', ',', '\\n', 'VIRGINIA', 'AND', 'KENTUCKY', 'RESOLUTIONS', 'OF', \"'\", '98', '-', \"-'99\", ',', '\\n', 'AND', '\\n', 'OTHER', 'ILLUSTRATIONS', 'OF', 'THE', 'CONSTITUTION', '.']\n",
      "['\\n']\n",
      "['IN', 'FOUR', 'VOLUMES', '.']\n",
      "['\\n', 'VOL', '.']\n",
      "['I.', '\\n', 'SECOND', 'EDITION', ',', 'WITH', 'CONSIDERABLE', 'ADDITIONS', '.']\n",
      "['\\n']\n",
      "['COLLECTED', 'AND', 'REVISED', 'FROM', 'CONTEMPORARY', 'PUBLICATIONS', ',', '\\n', 'BY', 'JONATHAN', 'ELLIOT', '.']\n",
      "['\\n', 'PUBLISHED', 'UNDER', 'THE', 'SANCTION', 'OF', 'CONGRESS', '.']\n",
      "['\\n']\n",
      "['WASHINGTON', ':', '\\n']\n",
      "['PRINTED', 'FOR', 'THE', 'EDITOR', '.']\n",
      "['\\n']\n",
      "['1836', '.']\n",
      "['\\n\\n', 'Entered', 'according', 'to', 'Act', 'of', 'Congress', ',', 'in', 'the', 'year', 'one', 'thousand', 'eight', 'hundred', '\\n', 'and', 'thirty', '-', 'six', ',', '\\n']\n",
      "['By', 'JONATHAN', 'ELLIOT', ',', '\\n']\n",
      "['In', 'the', 'Clerk', \"'s\", 'Office', 'of', 'the', 'District', 'Court', 'of', 'the', 'District', 'of', 'Columbia', '.']\n",
      "['DIGEST', 'OF', 'THE', 'CONSTITUTION', '.']\n",
      "['Wednesday', ',', 'June', '27', ',', '1787', '.']\n",
      "['\\n']\n",
      "['It', 'was', 'moved', 'and', 'seconded', 'to', 'postpone', 'the', 'consideration', 'of', 'the', '6th', 'resolution', 'reported', 'from', 'the', 'committee', ',', 'in', 'order', 'to', 'take', 'up', 'the', '7th', 'and', '8th', 'resolutions', '.']\n",
      "['\\n']\n",
      "['On', 'the', 'question', 'to', 'postpone', ',', 'it', 'passed', 'in', 'the', 'affirmative', '.']\n",
      "['\\n']\n",
      "['It', 'was', 'moved', 'and', 'seconded', 'to', 'agree', 'to', 'the', '1st', 'clause', 'of', 'the', '7th', 'resolution', ',']\n",
      "['namely:--', '\\n', '\"', 'Resolved', ',', 'That', 'the', 'right', 'of', 'suffrage', 'in', 'the', 'first', 'branch', 'of', 'the', 'national', 'legislature', 'ought', 'not', 'to', 'be', 'according', 'to', 'the', 'rule', 'established', 'in', 'the', 'Articles', 'of', 'Confederation', '.', '\"']\n",
      "['\\n', 'Before', 'a', 'determination', 'was', 'taken', 'on', 'the', 'clause', ',']\n",
      "['the', 'house', 'adjourned', 'till', 'to', '-', 'morrow', ',', 'at', '11', \"o'clock\", ',', 'A.', 'M.']\n"
     ]
    }
   ],
   "source": [
    "for doc in sentences:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
